mydata = spark.read.format("csv").option("header", "true").load("original.csv")
mydata.show()

from pyspark.sql.functions import *
mydata2 = mydata.withColumn("clean_city", when(mydata.City.isNull(), 'Unknown').otherwise(mydata.City))
# Filter the dataframe to only show data where job title is NOT NULL. 
mydata2 = mydata2.filter(mydata2.JobTitle.isNotNull())

#if salary is null replace it with mean value; first step change column attribute type from string to float 

mydata2 = mydata2.withColumn("clean_salary", mydata2.Salary.substr(2,100).cast('float'))

mean = mydata2.groupBy().avg('clean_salary')
mean.show()

mean = mydata2.groupBy().avg('clean_salary').take(1)[0][0]
print(mean)

from pyspark.sql.functions import lit
mydata2 = mydata2.withColumn('new_salary', when(mydata2.clean_salary.isNull(), lit(mean)).otherwise(mydata2.clean_salary))

# Populate the latitude or longitude with median value of the column

import numpy as np
latitudes = mydata2.select('Latitude')

# Cannot calculate median on data with NULL values
latitudes = latitudes.filter(latitudes.Latitude.isNotNull())

